\begin{abstractpage}
Massive point cloud data sets are currently being created and studied in
academia, the private sector, and the military. Many previous attempts at
rendering point clouds have allowed the user to visualize the data in a
three-dimensional way but did not allow them to interact with the data and would
require all data to be in memory at runtime. Recently, a few systems have
emerged that deal with real-time rendering of massive point clouds with
on-the-fly level of detail modification that handles out-of-core processing but
these systems have their own limitations. With the size and scale of massive
point cloud data coming from LiDAR (Light Detection and Ranging) systems, being
able to visualize the data as well as interact and transform the data is needed.

Previous work in out-of-core rendering
\cite{3_wenzel2014out,4_goswami_zhang_pajarola_gobbetti_2010,5_richter_2010}
showed that using Octrees and k-d trees can increase the availability of data as
well as allow a user to visualize the information in a much more useful manner.
However, viewing the data isn't enough; applying work in context-aware selection
\cite{2_yu:hal-01178051} and surface creation \cite{1_VAST:VAST11:105-112} the
visualization system would greatly benefit in usability and functionality.
\end{abstractpage}
